name: ðŸ¤– Ã‰valuation Automatique

on:
  push:
    branches: [ main, master ]
    # Ignorer les commits de feedback pour Ã©viter les boucles infinies
    paths-ignore: 
      - 'FEEDBACK.md'
  pull_request:
    branches: [ main, master ]

# Permissions nÃ©cessaires pour lire le repo et Ã©crire le feedback
permissions:
  contents: write
  pull-requests: write

jobs:
  evaluate:
    runs-on: ubuntu-latest
    name: ðŸ“ Ã‰valuation du Code
    
    steps:
    - name: ðŸ”„ Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: ðŸ”§ Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'

    - name: ðŸ“¦ Install MCP Evaluator
      run: |
        # Clone ou installer le serveur MCP d'Ã©valuation
        git clone https://github.com/votre-username/ia-workflow.git /tmp/evaluator
        cd /tmp/evaluator
        npm install
        npm run build

    - name: ðŸ“‹ Configure Evaluation Parameters
      id: config
      run: |
        # Lire les variables d'environnement ou utiliser les valeurs par dÃ©faut
        echo "competence=${{ vars.COMPETENCE || 'DÃ©veloppement Web' }}" >> $GITHUB_OUTPUT
        echo "bareme=${{ vars.BAREME || 'Respect des consignes (5pts), QualitÃ© du code (5pts), FonctionnalitÃ© (5pts), Bonnes pratiques (5pts)' }}" >> $GITHUB_OUTPUT
        echo "files_to_analyze=${{ vars.FILES_TO_ANALYZE || 'index.html,style.css,script.js' }}" >> $GITHUB_OUTPUT
        echo "niveau=${{ vars.NIVEAU || 'debutant' }}" >> $GITHUB_OUTPUT

    - name: ðŸ¤– Run MCP Evaluation
      id: evaluation
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        REPOSITORY_URL: ${{ github.server_url }}/${{ github.repository }}
        COMPETENCE: ${{ steps.config.outputs.competence }}
        BAREME: ${{ steps.config.outputs.bareme }}
        FILES_TO_ANALYZE: ${{ steps.config.outputs.files_to_analyze }}
        NIVEAU: ${{ steps.config.outputs.niveau }}
      run: |
        cd /tmp/evaluator
        
        # CrÃ©er un script d'interface pour le serveur MCP
        cat > evaluate.js << 'EOF'
        const { spawn } = require('child_process');
        const process = require('process');
        
        // Configuration de l'Ã©valuation
        const config = {
          repositoryUrl: process.env.REPOSITORY_URL,
          competence: process.env.COMPETENCE,
          bareme: process.env.BAREME,
          filesToAnalyze: process.env.FILES_TO_ANALYZE.split(',').map(f => f.trim()),
          niveau: process.env.NIVEAU
        };
        
        console.log('ðŸ”„ DÃ©marrage de l\'Ã©valuation...');
        console.log('Repository:', config.repositoryUrl);
        console.log('CompÃ©tence:', config.competence);
        console.log('Fichiers Ã  analyser:', config.filesToAnalyze);
        
        // Lancer le serveur MCP en mode Ã©valuation directe
        const mcpServer = spawn('node', ['dist/index.js'], {
          stdio: ['pipe', 'pipe', 'inherit'],
          env: { ...process.env }
        });
        
        // PrÃ©parer la requÃªte MCP
        const mcpRequest = {
          jsonrpc: '2.0',
          id: 1,
          method: 'tools/call',
          params: {
            name: 'evaluate_repository',
            arguments: config
          }
        };
        
        let output = '';
        
        mcpServer.stdout.on('data', (data) => {
          output += data.toString();
        });
        
        mcpServer.on('close', (code) => {
          if (code === 0) {
            console.log('âœ… Ã‰valuation terminÃ©e avec succÃ¨s');
            console.log('Output:', output);
          } else {
            console.error('âŒ Erreur lors de l\'Ã©valuation, code:', code);
            process.exit(1);
          }
        });
        
        // Envoyer la requÃªte au serveur MCP
        mcpServer.stdin.write(JSON.stringify(mcpRequest) + '\n');
        mcpServer.stdin.end();
        EOF
        
        # ExÃ©cuter l'Ã©valuation
        node evaluate.js

    - name: ðŸ“¤ Commit Feedback
      if: always()
      run: |
        # Configurer Git
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # VÃ©rifier si le fichier FEEDBACK.md a Ã©tÃ© crÃ©Ã©/modifiÃ©
        if [ -f "FEEDBACK.md" ]; then
          echo "ðŸ“ Fichier FEEDBACK.md trouvÃ©, commit en cours..."
          git add FEEDBACK.md
          git commit -m "ðŸ¤– Mise Ã  jour du feedback automatique

          Ã‰valuation effectuÃ©e automatiquement par GitHub Actions
          CompÃ©tence: ${{ steps.config.outputs.competence }}
          Niveau: ${{ steps.config.outputs.niveau }}
          
          GÃ©nÃ©rÃ©e le: $(date -u +'%Y-%m-%d %H:%M:%S UTC')"
          
          # Push avec retry en cas de conflit
          for i in {1..3}; do
            if git push; then
              echo "âœ… Feedback envoyÃ© avec succÃ¨s"
              break
            else
              echo "âš ï¸ Ã‰chec du push, tentative $i/3"
              git pull --rebase
              sleep $((i * 2))
            fi
          done
        else
          echo "âš ï¸ Aucun fichier FEEDBACK.md gÃ©nÃ©rÃ©"
        fi

    - name: ðŸ’¬ Comment on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          // Lire le contenu du feedback s'il existe
          const fs = require('fs');
          let feedbackContent = '';
          
          try {
            feedbackContent = fs.readFileSync('FEEDBACK.md', 'utf8');
          } catch (error) {
            feedbackContent = 'âš ï¸ Le feedback automatique n\'a pas pu Ãªtre gÃ©nÃ©rÃ©. Veuillez vÃ©rifier la configuration.';
          }
          
          // CrÃ©er un commentaire sur la PR
          const comment = `## ðŸ¤– Ã‰valuation Automatique
          
          ${feedbackContent.substring(0, 2000)}...
          
          ðŸ“„ **Voir le fichier FEEDBACK.md complet pour tous les dÃ©tails**
          
          ---
          *Ã‰valuation gÃ©nÃ©rÃ©e automatiquement par GitHub Actions*`;
          
          await github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

    - name: ðŸ“Š Summary
      if: always()
      run: |
        echo "## ðŸ“‹ RÃ©sumÃ© de l'Ã‰valuation" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **Repository:** ${{ github.repository }}" >> $GITHUB_STEP_SUMMARY
        echo "- **CompÃ©tence Ã©valuÃ©e:** ${{ steps.config.outputs.competence }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Niveau:** ${{ steps.config.outputs.niveau }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Fichiers analysÃ©s:** ${{ steps.config.outputs.files_to_analyze }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f "FEEDBACK.md" ]; then
          echo "âœ… **Status:** Ã‰valuation terminÃ©e avec succÃ¨s" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“ **Feedback:** Disponible dans le fichier FEEDBACK.md" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Status:** Erreur lors de l'Ã©valuation" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ”§ **Action:** VÃ©rifier la configuration et les logs" >> $GITHUB_STEP_SUMMARY
        fi
