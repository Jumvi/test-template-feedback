name: 🤖 Évaluation Automatique

on:
  push:
    branches: [ main, master ]
    # Ignorer les commits de feedback pour éviter les boucles infinies
    paths-ignore: 
      - 'FEEDBACK.md'
  pull_request:
    branches: [ main, master ]

# Permissions nécessaires pour lire le repo et écrire le feedback
permissions:
  contents: write
  pull-requests: write

jobs:
  evaluate:
    runs-on: ubuntu-latest
    name: 📝 Évaluation du Code
    
    steps:
    - name: 🔄 Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: 🔧 Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'

    - name: 📦 Install MCP Evaluator
      run: |
        # Clone ou installer le serveur MCP d'évaluation
        git clone https://github.com/votre-username/ia-workflow.git /tmp/evaluator
        cd /tmp/evaluator
        npm install
        npm run build

    - name: 📋 Configure Evaluation Parameters
      id: config
      run: |
        # Lire les variables d'environnement ou utiliser les valeurs par défaut
        echo "competence=${{ vars.COMPETENCE || 'Développement Web' }}" >> $GITHUB_OUTPUT
        echo "bareme=${{ vars.BAREME || 'Respect des consignes (5pts), Qualité du code (5pts), Fonctionnalité (5pts), Bonnes pratiques (5pts)' }}" >> $GITHUB_OUTPUT
        echo "files_to_analyze=${{ vars.FILES_TO_ANALYZE || 'index.html,style.css,script.js' }}" >> $GITHUB_OUTPUT
        echo "niveau=${{ vars.NIVEAU || 'debutant' }}" >> $GITHUB_OUTPUT

    - name: 🤖 Run MCP Evaluation
      id: evaluation
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        REPOSITORY_URL: ${{ github.server_url }}/${{ github.repository }}
        COMPETENCE: ${{ steps.config.outputs.competence }}
        BAREME: ${{ steps.config.outputs.bareme }}
        FILES_TO_ANALYZE: ${{ steps.config.outputs.files_to_analyze }}
        NIVEAU: ${{ steps.config.outputs.niveau }}
      run: |
        cd /tmp/evaluator
        
        # Créer un script d'interface pour le serveur MCP
        cat > evaluate.js << 'EOF'
        const { spawn } = require('child_process');
        const process = require('process');
        
        // Configuration de l'évaluation
        const config = {
          repositoryUrl: process.env.REPOSITORY_URL,
          competence: process.env.COMPETENCE,
          bareme: process.env.BAREME,
          filesToAnalyze: process.env.FILES_TO_ANALYZE.split(',').map(f => f.trim()),
          niveau: process.env.NIVEAU
        };
        
        console.log('🔄 Démarrage de l\'évaluation...');
        console.log('Repository:', config.repositoryUrl);
        console.log('Compétence:', config.competence);
        console.log('Fichiers à analyser:', config.filesToAnalyze);
        
        // Lancer le serveur MCP en mode évaluation directe
        const mcpServer = spawn('node', ['dist/index.js'], {
          stdio: ['pipe', 'pipe', 'inherit'],
          env: { ...process.env }
        });
        
        // Préparer la requête MCP
        const mcpRequest = {
          jsonrpc: '2.0',
          id: 1,
          method: 'tools/call',
          params: {
            name: 'evaluate_repository',
            arguments: config
          }
        };
        
        let output = '';
        
        mcpServer.stdout.on('data', (data) => {
          output += data.toString();
        });
        
        mcpServer.on('close', (code) => {
          if (code === 0) {
            console.log('✅ Évaluation terminée avec succès');
            console.log('Output:', output);
          } else {
            console.error('❌ Erreur lors de l\'évaluation, code:', code);
            process.exit(1);
          }
        });
        
        // Envoyer la requête au serveur MCP
        mcpServer.stdin.write(JSON.stringify(mcpRequest) + '\n');
        mcpServer.stdin.end();
        EOF
        
        # Exécuter l'évaluation
        node evaluate.js

    - name: 📤 Commit Feedback
      if: always()
      run: |
        # Configurer Git
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Vérifier si le fichier FEEDBACK.md a été créé/modifié
        if [ -f "FEEDBACK.md" ]; then
          echo "📝 Fichier FEEDBACK.md trouvé, commit en cours..."
          git add FEEDBACK.md
          git commit -m "🤖 Mise à jour du feedback automatique

          Évaluation effectuée automatiquement par GitHub Actions
          Compétence: ${{ steps.config.outputs.competence }}
          Niveau: ${{ steps.config.outputs.niveau }}
          
          Générée le: $(date -u +'%Y-%m-%d %H:%M:%S UTC')"
          
          # Push avec retry en cas de conflit
          for i in {1..3}; do
            if git push; then
              echo "✅ Feedback envoyé avec succès"
              break
            else
              echo "⚠️ Échec du push, tentative $i/3"
              git pull --rebase
              sleep $((i * 2))
            fi
          done
        else
          echo "⚠️ Aucun fichier FEEDBACK.md généré"
        fi

    - name: 💬 Comment on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          // Lire le contenu du feedback s'il existe
          const fs = require('fs');
          let feedbackContent = '';
          
          try {
            feedbackContent = fs.readFileSync('FEEDBACK.md', 'utf8');
          } catch (error) {
            feedbackContent = '⚠️ Le feedback automatique n\'a pas pu être généré. Veuillez vérifier la configuration.';
          }
          
          // Créer un commentaire sur la PR
          const comment = `## 🤖 Évaluation Automatique
          
          ${feedbackContent.substring(0, 2000)}...
          
          📄 **Voir le fichier FEEDBACK.md complet pour tous les détails**
          
          ---
          *Évaluation générée automatiquement par GitHub Actions*`;
          
          await github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

    - name: 📊 Summary
      if: always()
      run: |
        echo "## 📋 Résumé de l'Évaluation" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **Repository:** ${{ github.repository }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Compétence évaluée:** ${{ steps.config.outputs.competence }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Niveau:** ${{ steps.config.outputs.niveau }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Fichiers analysés:** ${{ steps.config.outputs.files_to_analyze }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f "FEEDBACK.md" ]; then
          echo "✅ **Status:** Évaluation terminée avec succès" >> $GITHUB_STEP_SUMMARY
          echo "📝 **Feedback:** Disponible dans le fichier FEEDBACK.md" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ **Status:** Erreur lors de l'évaluation" >> $GITHUB_STEP_SUMMARY
          echo "🔧 **Action:** Vérifier la configuration et les logs" >> $GITHUB_STEP_SUMMARY
        fi
